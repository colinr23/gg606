[
["index.html", "Scientific Data Wrangling Chapter 1 Introduction 1.1 Textbooks 1.2 Cheatsheets 1.3 Additional Readings 1.4 Schedule 1.5 Evaluation", " Scientific Data Wrangling Colin Robertson 2022-02-10 Chapter 1 Introduction Class Time: Thursdays, 2:30 - 5:20, Peters Building P331 Moving from data acquired by a sensor or in the field to a model or visualization that can provide insights to a question often requires an extensive amount of work. It is estimated that ‘data wrangling’ - cleaning, loading, processing, integrating their data comprises at least half of a data scientist’s time, and that may be even higher in the context of environmental data science. The official course outline is available here 1.1 Textbooks Wickham H, Grolemund G. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media. Chicago, available online at http://r4ds.had.co.nz/ Timbers, T-A., Campbell, T., and Lee, M. 2021. Introducation to Data Science https://ubc-dsci.github.io/introduction-to-datascience/ 1.2 Cheatsheets Use these R-Studio Cheatsheets 1.3 Additional Readings Broman KW, Woo KH. 2018. Data Organization in Spreadsheets. The American Statistician 72(1): 2-10. https://doi.org/10.1080/00031305.2017.1375989 Bryan J, et al. 2018. Happy Git and GitHub for the useR. http://happygitwithr.com/ Hampton SE, Anderson SS, Bagby SC, Gries C, Han X, Hart EM, Jones MB, Lenhardt WC, MacDonald A, Michener WK, Mudge J, Pourmokhtarian A, Schildhauer MP, Woo KH, Zimmerman N. 2015. The Tao of open science for ecology. Ecosphere 6(7):120. http://dx.doi.org/10.1890/ES14-00402.1 Hart EM, Barmby P, LeBauer D, Michonneau F, Mount S, Mulrooney P, et al. 2016. Ten Simple Rules for Digital Data Storage. PLoS Comput Biol12(10): e1005097. https://doi.org/10.1371/journal.pcbi.1005097 Sixteen peer-reviewed journal articles in the PeerJ Collection, Practical Data Science for Stats: https://peerj.com/collections/50-practicaldatascistats/ Wilke CO. 2019. Fundamental of Data Visualization: A Primer on Making Informative and Compelling Figures. O’Reilly Media. Chicago, https://serialmentor.com/dataviz/ 1.4 Schedule Wrangling Week Topic Tools Jan 6 Data/Science workflows R, R-Studio Jan 13 i/o R (base, tidyverse) Jan 20 Data objects R (tidyverse) Jan 27 Databases R (tidyverse, dbply), SQLite Feb 03 Spatial data R (rgdal, sp, sf, raster, terra) Feb 10 Spatial data R (spatstat, gstat) Feb 17 Work Period/Catch up Feb 24 Reading Week Mar 03 Temporal data R (ts, zoo, lubridate) Data Science in Practice Week Topic Tools Mar 10 Reproducible Research Git, Github Mar 17 Data Sharing and Collaboration R Markdown, Docker Mar 24 Project Work Period Mar 31 R in Production Docker, R-Shiny Apr 07 Project Presentations n/a 1.5 Evaluation Evaluation in the course will be comprised of a mix of hands-on components designed to build skills in environmental data science. There will be two assignments and one project required for the course. Students will be required to give a technical demonstration of their work and a project write-up. 1.5.1 Grading Assessment Component Weighting Due Date Assignments (2X15%) 30% Feb 10, Mar 03 Analytics Studio Demo 40% Mar 31, Apr 07 Participation 15% N/A Course notebook 15% Apr 07 Total: 100% "],
["intro.html", "Chapter 2 Wrangling 2.1 Data Science Workflows 2.2 i/o 2.3 Data objects 2.4 Databases 2.5 Spatial data 2.6 Temporal data", " Chapter 2 Wrangling 2.1 Data Science Workflows 2.1.1 Reading This week will provide an introduction to data science using r. You will be introduced to data science through the lens of exploratory data analysis, beginning with the Explore section of the textbook, read and work through the following chapters: W&amp;G - Introduction W&amp;G - Data visualization W&amp;G - Data Transformation W&amp;G - Workflow: basics W&amp;G - Workflow: scripts W&amp;G - Workflow: projects 2.1.2 Excercise Review Data Visualization: 1, 2, 3, 4, 5, 6 Workflow: basics: 1 2.2 i/o 2.2.1 Reading This week will cover the surprisingly important topic of data i/o - that is, data importing and outputting data. In the Wrangle section of the textbook, read and work through the following chapters: JHDSc - Data import W&amp;G - Tidy data As well as the paper describing the tidy approach to data, published in the Journal of Statistical Software. R documentation - Review of Data Import/Export Data sources and formats vary widely in environmental data analytics. Data acquisition is the steps taken to access and load data for your own analysis. In a classic analysis workflow architecture, data are downloaded to a local working computer and interact with software installed there to complete your analysis. Raw data almost always has to be processed in different ways to make it useful. This can in include Quality Assurance/Quality Control (QA/QC) steps such as checking and removing duplicate records, cleaning out special characters from text, to things like date formatting and computing derived variables. Having a consistent way to generate data transformations is a cornerstone of reproducible research and modern scientific data workflows. This is one of the key reasons scripting has become so important to environmental data analytics. The tidy approach to data science aims to provide a cohesive and consistent approach to data organization in r. 2.3 Data objects 2.3.1 Reading We will cover some more basic elements of the R programming language - both at the base and more modern levels. At some point having an understanding of how R works at a deeper level becomes necessary to get the most out of it and develop effective analysis and visualization approaches. Wickham - Vectors TCL - Wrangling 2.4 Databases 2.4.1 Resources There are no dedicated readings this week, but below are some of the resources discussed during class this week. R-studio docs Examples of lazy execution 2.5 Spatial data Spatial data wrangling encompasses a set of techniques usually found in standard GIS courses. Tasks such as spatial query, spatial overlay, combining vector and raster data, are all often necessary steps in preparing spatial data for analysis. Due to the mature collection of packages for working with spatial data n r, all of these operations can be performed in r, generally through a tidy workflow. We will review some of the more common spatial wrangling tasks this week through a tutorial. 2.5.1 Reading 501 Lecture slides for review Pebezma 2018 Pebezma - geom Pebezma - sf Gimond Appendix- Walk-thru of common spatial operations 2.5.1.1 Point in polygon counts Data: point event crime in waterloo, polygon census units in waterloo Task: for a given type of crime, subset and count occurences within each census unit Operation: Spatial query/subsetting Read data in: ## [1] TRUE ## Reading layer `lct_000b16a_e&#39; from data source `/Users/colinr23/Dropbox/course/g606/tec/gg606/data/lct_000b16a_e.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 5721 features and 8 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 3912489 ymin: 693729 xmax: 8996156 ymax: 2810986 ## projected CRS: PCS_Lambert_Conformal_Conic The Region of Waterloo has an open data we can access some other datasets from: We now have ct which is an sf class which represents census tracts in the Waterloo area. 2.5.1.2 Read in our three spatial data files ## Reading layer `Trails&#39; from data source `/Users/colinr23/Dropbox/course/g606/tec/gg606/data/Trails/Trails.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 1633 features and 28 fields ## geometry type: MULTILINESTRING ## dimension: XY ## bbox: xmin: -80.81168 ymin: 43.26102 xmax: -80.27309 ymax: 43.65101 ## geographic CRS: WGS 84 ## Reading layer `Park_Natural_Area_Ecological_Land_Classification&#39; from data source `/Users/colinr23/Dropbox/course/g606/tec/gg606/data/Park_Natural_Area_Ecological_Land_Classification/Park_Natural_Area_Ecological_Land_Classification.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 625 features and 12 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -80.55806 ymin: 43.37734 xmax: -80.40301 ymax: 43.50665 ## geographic CRS: WGS 84 ## [1] &quot;PCS_Lambert_Conformal_Conic&quot; ## [1] &quot;+proj=lcc +lat_0=63.390675 +lon_0=-91.8666666666667 +lat_1=49 +lat_2=77 +x_0=6200000 +y_0=3000000 +datum=NAD83 +units=m +no_defs&quot; ## [1] &quot;WGS 84&quot; ## [1] &quot;+proj=longlat +datum=WGS84 +no_defs&quot; ## [1] &quot;WGS 84&quot; ## [1] &quot;+proj=longlat +datum=WGS84 +no_defs&quot; ## [1] 4326 Here we can make some further observations of our data. Our reprojection worked as the dare plotting correctly The trails layer exceeds the boundaries of the ct layer There are no eco polygons in the outer areas of the cts We can clip the trails layer to the bounds of the ct layer. It might be useful to generate a study area polygon based on the union of the polygons in ct. We saw last week we can do this using dplyr syntax for grouping studyArea &lt;- ct %&gt;% group_by() %&gt;% summarise() plot(st_geometry(studyArea)) which we can now provide to a clip operation.. trails_clip &lt;- st_intersection(trails, studyArea) ## Warning: attribute variables are assumed to be spatially constant throughout all ## geometries #we can inspect the attributes to make sure names(trails) ## [1] &quot;OBJECTID&quot; &quot;TrailType&quot; &quot;TrailName&quot; &quot;Streetname&quot; &quot;Municipali&quot; ## [6] &quot;Settlement&quot; &quot;OwnedBy&quot; &quot;Maintained&quot; &quot;OnOffRoad&quot; &quot;SurfaceMat&quot; ## [11] &quot;Obstructio&quot; &quot;InstallDat&quot; &quot;Width_m&quot; &quot;Length_m&quot; &quot;Notes&quot; ## [16] &quot;LifeStatus&quot; &quot;RMWID&quot; &quot;CreatedUse&quot; &quot;CreatedDat&quot; &quot;LastEdited&quot; ## [21] &quot;LastEdit_1&quot; &quot;LucityAuto&quot; &quot;InLucity&quot; &quot;LastSyncDa&quot; &quot;SegmentLuc&quot; ## [26] &quot;SHAPESTLen&quot; &quot;ContractNu&quot; &quot;Comments&quot; &quot;geometry&quot; Examining these, we probably do not want to rely on Length_m in trails_clip since those lengths are going to refer to the length of the trial prior to us clipping them by the ct geometry. This is not a huge problem because we can extract geometric properties like length automatically from the geometry of the layer itself: #always good to do some spot checking when wrangling head(st_length(st_geometry(trails))) ## Units: [m] ## [1] 139.3061 378.1244 680.8490 450.6546 333.1547 495.0084 head(trails$Length_m) ## [1] 139.3061 378.1244 680.8490 450.6546 333.1547 495.0084 head(st_length(st_geometry(trails_clip))) ## Units: [m] ## [1] 139.3061 378.1244 680.8490 450.6546 333.1547 495.0084 head(trails_clip$Length_m) ## [1] 139.3061 378.1244 680.8490 450.6546 333.1547 495.0084 Now we see we are getting identical results, but we have to remember only a few trails (i.e., those at the edges) are going to be affected by this issue. The safe thing to do would be do ignore Length_m in the table and just use the result of st_length in any analysis. BUT for the purposes of illustration let’s explore where the discrepancy exists. We want to find rows in trails_clip have a different value for Length_m than the corresponding rows in trails. It is also possible that some entire features were clipped out - we can check this by simpling comparing the number of features/rows in each: nrow(trails)==nrow(trails_clip) ## [1] FALSE Where we can see that in fact there are less features in trails_clip. If we look at how data are structured in the trails data we can learn more about how things are stored: head(trails) ## Simple feature collection with 6 features and 28 fields ## geometry type: MULTILINESTRING ## dimension: XY ## bbox: xmin: 536694.7 ymin: 4801974 xmax: 556613.6 ymax: 4814768 ## projected CRS: NAD83 / UTM zone 17N ## OBJECTID TrailType TrailName ## 1 1 Multi-Use &lt;NA&gt; ## 2 2 Multi-Use Trans Canada Trail \\\\ Iron Horse Trail ## 3 3 Multi-Use Trans Canada Trail \\\\ Iron Horse Trail \\\\ Laurel Trail ## 4 4 Multi-Use Trans Canada Trail ## 5 5 Multi-Use Trans Canada Trail \\\\ Laurel Trail ## 6 6 Multi-Use Trans Canada Trail ## Streetname Municipali Settlement OwnedBy Maintained OnOffRoad ## 1 &lt;NA&gt; Cambridge Cambridge Cambridge Cambridge Off ## 2 &lt;NA&gt; Waterloo Waterloo Waterloo Waterloo Off ## 3 &lt;NA&gt; Waterloo Waterloo Waterloo Waterloo Off ## 4 University Ave W Waterloo Waterloo Waterloo Waterloo Off ## 5 University Ave W Waterloo Waterloo Waterloo Waterloo Off ## 6 &lt;NA&gt; Waterloo Waterloo Waterloo Waterloo Off ## SurfaceMat Obstructio InstallDat Width_m Length_m ## 1 Asphalt &lt;NA&gt; &lt;NA&gt; 3 139.3061 ## 2 Asphalt &lt;NA&gt; &lt;NA&gt; 3 378.1244 ## 3 Asphalt &lt;NA&gt; &lt;NA&gt; 3 680.8490 ## 4 Asphalt &lt;NA&gt; &lt;NA&gt; 3 450.6546 ## 5 Asphalt &lt;NA&gt; &lt;NA&gt; 3 333.1547 ## 6 Stone Chips &lt;NA&gt; &lt;NA&gt; 3 495.0084 ## Notes ## 1 Tunnel under rail connecting Elgin St N; Dismount bike sign, PATHWAY, SHORT TERM 2008-2012 ## 2 &lt;NA&gt; ## 3 Laurel Trail ## 4 &lt;NA&gt; ## 5 Laurel Trail ## 6 &lt;NA&gt; ## LifeStatus RMWID CreatedUse CreatedDat LastEdited LastEdit_1 LucityAuto ## 1 In Service TRA1 &lt;NA&gt; 2011-08-25 RUCHRIS 2019-08-14 515 ## 2 In Service TRA2 &lt;NA&gt; 2011-09-01 RUCHRIS 2019-08-14 516 ## 3 In Service TRA3 &lt;NA&gt; 2011-09-01 RUCHRIS 2019-08-14 517 ## 4 In Service TRA4 &lt;NA&gt; 2011-09-01 RMW 2018-11-05 518 ## 5 In Service TRA5 &lt;NA&gt; 2011-09-01 RMW 2018-11-05 519 ## 6 In Service TRA6 &lt;NA&gt; 2011-09-01 RUCHRIS 2019-08-14 520 ## InLucity LastSyncDa SegmentLuc SHAPESTLen ContractNu Comments ## 1 1 2019-08-14 NA 139.3061 &lt;NA&gt; &lt;NA&gt; ## 2 1 2019-08-14 NA 378.1244 &lt;NA&gt; &lt;NA&gt; ## 3 1 2019-08-14 NA 680.8490 &lt;NA&gt; &lt;NA&gt; ## 4 1 2018-11-05 8693 450.6546 &lt;NA&gt; &lt;NA&gt; ## 5 1 2018-11-05 8693 333.1547 &lt;NA&gt; &lt;NA&gt; ## 6 1 2019-08-14 NA 495.0084 &lt;NA&gt; &lt;NA&gt; ## geometry ## 1 MULTILINESTRING ((556575.6 ... ## 2 MULTILINESTRING ((536694.7 ... ## 3 MULTILINESTRING ((536731.5 ... ## 4 MULTILINESTRING ((537060.2 ... ## 5 MULTILINESTRING ((537371.8 ... ## 6 MULTILINESTRING ((537627 48... Looking at the TrailName column, we can see that this is actually fairly complex data. Each feature is not necessarily a complete trail, and even the same section of trail may have multiple names (e.g., the Iron Horse Trail is also considered part of the Trans Canada Trail). This model also ensures that the geometry is not duplicated in these sections, although it may make summarizing and some types of analysis more difficult. In order to answer our original question - which features in trails had lengths changed and as a result the values in Length_m in trails_clip are no longer accurate. To be safe, we can simple recompute the lengths using the st_length function trails_clip$Length_m &lt;- st_length(trails_clip) head(trails$Length_m) ## [1] 139.3061 378.1244 680.8490 450.6546 333.1547 495.0084 head(trails_clip$Length_m) ## Units: [m] ## [1] 139.3061 378.1244 680.8490 450.6546 333.1547 495.0084 which we can see are equivalent, however notice that the output of the st_length function are aware of their units. This is dependent on the projection metadata and can be very useful. However,some functions may not know how to deal with data of this type (check class(traisclip$Length_m) in which case you can wrap it in as.numeric to convert to standard numeric vector data. 2.5.1.3 Measuring the length of lines within polygons One thing we may want to do is calculate the total length of trails in each census tract. Now that we know that where we have overlapping trails (i.e., segments of more than one trail overlapping) that these are stored as a single linear feature not multiple, we can simply intersect the trails_clip data with the census tract and then summarize by census tract. #### Average attirbute value of point within polygons 2.5.1.4 Average value of a raster within polygons 2.5.1.5 Extract value of raster to points as new attribute 2.5.1.6 Downsample a raster 2.5.1.7 Interpolate a surface and average over polygons 2.5.1.8 Dissovle boundaries between adjacent polygons 2.5.1.9 Calcuate a point pattern kensity estimate 2.6 Temporal data "],
["practice.html", "Chapter 3 Practice", " Chapter 3 Practice "],
["assignments.html", "Chapter 4 Assignments 4.1 Assignment 1 - Importing, parsing, and querying data in the wild 4.2 Assignment 2 - Real world data wrangling", " Chapter 4 Assignments 4.1 Assignment 1 - Importing, parsing, and querying data in the wild Your objective in this assignment is to read in, clean, and process a dataset. The due date of this assignment has shifted to Feb 10. 4.1.1 Dataset The National Earthquake Information Center (NEIC) determines the location and size of all significant earthquakes that occur worldwide and disseminates this information immediately to national and international agencies, scientists, critical facilities, and the general public. The NEIC compiles and provides to scientists and to the public an extensive seismic database that serves as a foundation for scientific research through the operation of modern digital national and global seismograph networks and cooperative international agreements. The NEIC is the national data center and archive for earthquake information. This dataset includes a record of the date, time, location, depth, magnitude, and source of every earthquake with a reported magnitude 5.5 or higher since 1965. You have to structure your assignment as a r mardown file. Watch this video (~45 mins) to learn how to start and navigate markdown files in R-Studio. Disclaimer: I am not endorsing buying these courses - this is just a straightforward introduction to get up to up and running with R-markdown. You can use the following as a template [download to your working directory, and open in R-studio to edit/knit] to get you started for your assignment. Your assignment should answer the following questions and be as reproducible as you can make it (i.e., I should be able to reproduce your answers).This means that you must read data in from a URL so that I can replicate your work, do not include any external data files in your submission, only submit one .Rmd file. You can use external data to supplement your anlayses if you want to. For each answer provide a short write up explaining the approach you took to the question. There are not necessarily correct answers, and I expect your answers to vary from classmates, however you should be able to provide a clear illustration via your code of how you arrived at the conclusion you did. 4.1.2 Questions Read the data in and clean it for analysis, used the readr package functions for reading and parsing data. [5 marks] Did more earthquakes happen on weekends or weekdays? [5 marks] Has there been any change in the frequency of earthquakes? [5 marks] What had more earthquakes in the 1980s, South America or North America? [5 marks] Has there been any geographic shifts in the distribution of earthquakes? [10 marks] 4.1.3 Submission Submit via email to Colin at the start of class on Feb 10 4.2 Assignment 2 - Real world data wrangling The Canadian Census from 2021 will soon be released and made available to the public. You will analyze Canadian census data for this assignment. However, the real goal of this assignment is to get you familiar with the process of learning a new r package. More than anything - the r landscape of packages is quickly changing and being able to learn about, understand, and use new packages is a vital skill for scientific data wrangling. Often, new published papers will have relatedr packages and may or may not have clear documentaiton or vignetttes (which are best for getting up and running). Evaluating an r package requires quickly ascertaining whether a package can do what you need it to, how to format data for it, what outputs are generated, and what parameters need to be set/configured. There is very little standardization across r packages (outside of the tidyverse) so this step of evaluation can take some time. Your goal for this assignment is to get up and running with the cancensus package. You can learn more about the cancensus package here. To submit for this assignment: a reproducible R markdown file that develops an analysis of census data for a geographical location in Canada of interest to you. You are free to incorporate external data from other sources if you wish, but the focus should be on data that are in the census. The geography of interest must meet the following criteria: has a name that starts with same letter as your first name or last name is comprised of at least 30 geographic units is somewhere that you have not personally visited The analysis may focus on traditional census themes like population change or dive deep into more specific demographic or regional questions. Your analysis should present a coherent data story, and should mix visualizaitons and written interpretations of your analysis. You must include all r code for reprorducing your report, however do not have to show all of the code in the output in the final report (i.e. you can have echo=FALSE in some of your code chunks if you want them hidden from the output - learn how to use chunk options in R-studio). Focus on quality over quantity, only include analysis which contributes to your overally narrative, do not include every type of graph or model you explored. "],
["term-project-analytics-studio-demo.html", "Chapter 5 Term Project - Analytics Studio Demo 5.1 Overview 5.2 Submission", " Chapter 5 Term Project - Analytics Studio Demo 5.1 Overview The goal of this project is to demonstrate a technical topic to an audience in an interesting way. Being able to communicate technical details in accessible ways is a critical skill for working with collaborators on scientific projects. People need to know what you did, what decisions were made and why, how they affected the outcome, and potential issues or shortcomings in the approach taken. While peer-review is one part of the scientific knowledge production process - increasingly it is not sufficient to just describe your methods in a paper, but these must be presented as supplementary code or a notebook which documents exact data processing steps. Your task for the term project is to provide a complete analytic walk-thru of an existing analysis from a paper in an area of interest (i.e., replicated by you) or through a demonstration of a specialized statistical or analysis method through the use of r packages. If you are demoing a method this must be a completely new demonstration and ideally will compare and contrast multiple packages, not simply a rehashing of an existing tutorial or vignette. Please consult with me to confirm your chosen topic. There are two fundamental parts to this projet, demonstration, and critique. 5.1.1 Demonstration In this part of the project your goal is to articulate as clearly as possible to a scientific but non-specialized audience the full scope of your analysis. This should comprise about half of your written report and about 70% of your presentation/overview in class. 5.1.2 Critique In this part of the project you should critically analyze the analysis and/or methods implemented in the package. Focus here on issues of data quality, uncertainty, key parameters, workflow, etc. Your aim here is to provide a critical overview of the methods and analysis presented so as to provide guidance and advice to scientific collaborators. 5.2 Submission The term project here is comprised of a written report and an in-class demonstration (i.e., delivered via zoom for remote participants). The report will be worth 65 points according to the following breakdown: Technical depth - /40 Critique - /25 Accuracy - /20 Writing style - /15 The presentation will be worth 35 points and graded according to the following breakdown: Aesthetic appeal - /25 Clarity and communication style - /25 Technical completeness - /50 The report should be no longer than 4000 words. The presentation should be between 13-15 minutes. The presentation file will not be submitted for grading. "]
]
